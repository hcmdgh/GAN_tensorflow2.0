{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:56.978556Z",
     "start_time": "2020-02-19T14:45:56.973570Z"
    }
   },
   "source": [
    "# CIFAR10andResNet18\n",
    "\n",
    "## ResNet原理\n",
    "\n",
    "ResNet 通过在卷积层的输入和输出之间添加Skip Connection 实现层数回退机制，如下图所示，输入$x$通过两个卷积层，得到特征表换后的输出$F(x)$，与输入$x$进行对应元素的相加运算，得到最终输出$H(x)$:\n",
    "\n",
    "$$H(x)=x+F(x)$$\n",
    "\n",
    "$H(x)$叫作残差模块(Residual Block，简称ResBlock)。由于被 Skip Connection 包围的卷积神 经网络需要学习映射$F(x)=H(x)-x$，故称为残差网络。\n",
    "\n",
    "<img src=\"ResBlock.PNG\" width=\"40%\">\n",
    "\n",
    "为了能够满足输入$x$与卷积层的输出$F(x)$能够相加运算，需要输入$x$的shape与$F(x)$的shape完全一致。当出现shape不一致时，一般通过在 Skip Connection 上添加额外的卷积运算环节将输入$x$变换到与$F(x)$相同的shape，如图 10.63 中$identity(x)$函数所示，其中$identity(x)$以 $1*1$的卷积运算居多，主要用于调整输入的通道数。\n",
    "\n",
    "这里的相加计算指：\n",
    "\n",
    "两个\\[n,h,r,c\\]的网络相加得到一个\\[n,h,r,c\\]的网络。\n",
    "\n",
    "## ResNet18\n",
    "\n",
    "本页将实现18层的**深度残差网络**ResNet18，并在CIFAR10图片集上训练与测试。\n",
    "\n",
    "标准的ResNet18接受输入为224*224大小的图片数据，我们将ResNet18 进行适量调整，使得它输入大小为32 × 32，输出维度为10。调整后的ResNet18网络结构如下图。\n",
    "\n",
    "<img src=\"ResNet.PNG\">\n",
    "\n",
    "## 实现\n",
    "\n",
    "首先实现中间两个卷积层，Skip Connection 1\\*1卷积层的残差模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:50:58.431398Z",
     "start_time": "2020-02-21T03:50:52.111794Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Sequential,optimizers,losses,datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "    #残差模块\n",
    "    def __init__(self,filter_num,stride=1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        #第一个卷积单元\n",
    "        self.conv1=layers.Conv2D(filter_num,(3,3),strides=stride,padding='SAME')\n",
    "        self.bn1=layers.BatchNormalization()\n",
    "        self.relu=layers.Activation('relu')\n",
    "        #第二个卷积单元\n",
    "        self.conv2=layers.Conv2D(filter_num,(3,3),strides=1,padding='SAME')\n",
    "        self.bn2=layers.BatchNormalization()\n",
    "        \n",
    "        if stride != 1:#通过1*1卷积完成shape匹配\n",
    "            self.downsample=Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num,(1,1),strides=stride))\n",
    "        else:#shape匹配，直接短接\n",
    "            self.downsample=lambda x:x\n",
    "        \n",
    "    def call(self,inputs,training=None):\n",
    "        #前向计算函数\n",
    "        #[b,h,w,c]，通过第一个卷积单元\n",
    "        out=self.conv1(inputs)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "        #通过第二个卷积单元\n",
    "        out=self.conv2(out)\n",
    "        out=self.bn2(out)\n",
    "        #通过identity模块\n",
    "        identity=self.downsample(inputs)\n",
    "        #2条路径输出直接相加\n",
    "        output=layers.add([out,identity])\n",
    "        output=tf.nn.relu(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在设计深度卷积神经网络时，一般按照特征图高宽ℎ/𝑤逐渐减少，通道数𝑐逐渐增大的经验法则。可以通过堆叠通道数逐渐增大的ResBlock来实现高层特征的提取，通过build_resblock可以一次完成多个残差模块的新建。\n",
    "\n",
    "下面实现通用的ResNet 网络模型\n",
    "\n",
    "补充：\n",
    "\n",
    "help(layers.GlobalAveragePooling2D)\n",
    "\n",
    "    GlobalAveragePooling2D(data_format=None, **kwargs)\n",
    "    |  \n",
    "    |  Global average pooling operation for spatial data.\n",
    "    |  \n",
    "    |  Arguments:\n",
    "    |      data_format: A string,\n",
    "    |  \n",
    "    |  Input shape:\n",
    "    |    - If `data_format='channels_last'`:(default)\n",
    "    |      4D tensor with shape `(batch_size, rows, cols, channels)`.\n",
    "    |    - If `data_format='channels_first'`:\n",
    "    |      4D tensor with shape `(batch_size, channels, rows, cols)`.\n",
    "    |  \n",
    "    |  Output shape:\n",
    "    |    2D tensor with shape `(batch_size, channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:01.982433Z",
     "start_time": "2020-02-21T03:51:01.967472Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(keras.Model):\n",
    "    #通用的ResNet实现类\n",
    "    \n",
    "    def __init__(self,layer_dims,num_classes=10):#[2,2,2,2]\n",
    "        super(ResNet,self).__init__()\n",
    "        #根网络，预处理\n",
    "        self.stem=Sequential([\n",
    "            layers.Conv2D(64,(3,3),strides=(1,1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding='SAME')\n",
    "            ])\n",
    "        \n",
    "        #堆叠4个Block，每个Block包含了多个BasicBlock，设置步长不一样\n",
    "        self.layer1=self.build_resblock(64,layer_dims[0])\n",
    "        self.layer2=self.build_resblock(128,layer_dims[1],stride=2)\n",
    "        self.layer3=self.build_resblock(256,layer_dims[2],stride=2)\n",
    "        self.layer4=self.build_resblock(512,layer_dims[3],stride=2)\n",
    "        \n",
    "        #通过Pooling层将高宽降低为1*1\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        #最后连接成一个全连接层分类\n",
    "        self.fc=layers.Dense(num_classes)\n",
    "        \n",
    "    def call(self,inputs,training=None):\n",
    "        #前向计算函数：通过根网络\n",
    "        x=self.stem(inputs)\n",
    "        #一次通过4个模块\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        #通过池化层\n",
    "        x=self.avgpool(x)\n",
    "        #通过全连接层\n",
    "        x=self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build_resblock(self,filter_num,blocks,stride=1):\n",
    "        #辅助函数，堆叠filter_num个BasicBlock\n",
    "        res_blocks=Sequential()\n",
    "        #只有第一个 BasicBlock 的步长可能不为1，实现下采样\n",
    "        res_blocks.add(BasicBlock(filter_num,stride))\n",
    "        \n",
    "        for _ in range(1,blocks):#其他BasicBlock步长都为1\n",
    "            res_blocks.add(BasicBlock(filter_num,stride=1))\n",
    "    \n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调整每个Res Block的堆叠数量和通道数可以产生不同的ResNet，如通过64-64-128-128-256-256-512-512通道数配置，共8个ResBlock，可得到ResNet18的网络模型。每个 ResBlock 包含了2个主要的卷积层，因此卷积层数量是8∙2=16，加上网络末尾的全连接层，共18层。创建ResNet18和ResNet34可以简单实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:03.861797Z",
     "start_time": "2020-02-21T03:51:03.855814Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    return ResNet([2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet([3,4,6,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练并保存网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:15:02.875774Z",
     "start_time": "2020-02-21T02:18:00.041399Z"
    }
   },
   "outputs": [],
   "source": [
    "model=resnet18()\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()\n",
    "optimizer = optimizers.Adam(lr=1e-4) # 构建优化器\n",
    "\n",
    "#%%\n",
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "#删除y的一个维度,[b,1]=>[b]\n",
    "y=tf.squeeze(y,axis=1)\n",
    "y_test=tf.squeeze(y_test,axis=1)\n",
    "#打印训练接和测试集的形状\n",
    "print(x.shape,y.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # 将数据映射到-1~1\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32) # 类型转换\n",
    "    return x,y\n",
    "\n",
    "#构建训练集对象，随机打乱，预处理，批量化\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_db=train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "#构建测试集对象，预处理，批量化\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "test_db=test_db.map(preprocess).batch(128)\n",
    "#从训练集中采用一个Batch，并观察\n",
    "sample=next(iter(train_db))\n",
    "print('sample:',sample[0].shape,sample[1].shape,tf.reduce_min(sample[0]),tf.reduce_max(sample[0]))\n",
    "\n",
    "#%%\n",
    "#模型计算\n",
    "def main():\n",
    "    for epoch in range(5):\n",
    "        for step,(x,y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                #[b,32,32,3] => [b,10]\n",
    "                logits=model(x)\n",
    "                #[b] => [b,10]\n",
    "                y_onehot=tf.one_hot(y,depth=10)\n",
    "                #compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot,logits,from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            grads = tape.gradient(loss,model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "            if step%100==0:\n",
    "                print(epoch,step,'loss:',float(loss))\n",
    "                \n",
    "        total_num=0\n",
    "        total_corret=0\n",
    "        for x,y in test_db:\n",
    "            logits=model(x)\n",
    "            prob=tf.nn.softmax(logits,axis=1)\n",
    "            pred=tf.argmax(prob,axis=1)\n",
    "            pred=tf.cast(pred,dtype=tf.int32)\n",
    "            \n",
    "            correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "            correct=tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num+=x.shape[0]\n",
    "            total_corret+=int(correct)\n",
    "        \n",
    "        acc=total_corret/total_num\n",
    "        print(epoch,'acc:',acc)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.save('ResNet18.h5',save_format=\"tf\") \n",
    "print('saving ResNet18')\n",
    "```\n",
    "会报错：\n",
    "\n",
    "    NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
    "\n",
    "因为自定义网络不能直接保存整个网络，而可以采用保存权值的方式。\n",
    "\n",
    "这种保存与加载网络的方式最为轻量级，文件中保存的仅仅是张量参数的数值，并没有其 它额外的结构参数。但是它需要使用相同的网络结构才能够正确恢复网络状态，因此一般 在拥有网络源文件的情况下使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:45:13.728718Z",
     "start_time": "2020-02-21T03:45:13.359672Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights('ResNet18_weights.ckpt') \n",
    "print('saved weights.')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:13.486583Z",
     "start_time": "2020-02-21T03:51:10.640442Z"
    }
   },
   "outputs": [],
   "source": [
    "model=resnet18()\n",
    "model.load_weights('ResNet18_weights.ckpt') \n",
    "print('loaded weights!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出图片识别结果\n",
    "\n",
    "以下部分同：CIFAR10andVGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:13.495560Z",
     "start_time": "2020-02-21T03:51:13.488578Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer=dict({0:\"airplane\",\n",
    "        1:\"automobile\",\n",
    "        2:\"bird\",\n",
    "        3:\"cat\",\n",
    "        4:\"deer\",\n",
    "        5:\"dog\",\n",
    "        6:\"frog\",\n",
    "        7:\"horse\",\n",
    "        8:\"ship\",\n",
    "        9:\"truck\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:13.586327Z",
     "start_time": "2020-02-21T03:51:13.500547Z"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def test(file_name):\n",
    "    image_value=imageio.imread(file_name)[::,::,0:3]#因为读进来时是四通道\n",
    "    plt.imshow(image_value)\n",
    "    image_value=tf.image.resize(image_value,[32,32],antialias=True)\n",
    "    image_value=tf.expand_dims(image_value,axis=0)\n",
    "    image_value=2*tf.cast(image_value, dtype=tf.float32) / 255. - 1\n",
    "    logits=model(image_value)\n",
    "    res=tf.argmax(logits,axis=1)\n",
    "    plt.xlabel(transfer[int(res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:19.395143Z",
     "start_time": "2020-02-21T03:51:13.956330Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name=\"cat.jpg\"\n",
    "test(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
